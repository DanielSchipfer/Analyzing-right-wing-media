{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time \n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seite:\n",
    "    def __init__(self,url):\n",
    "        self.url = url \n",
    "        self.folderPath = \"data/\"\n",
    "        self.isArticle = None\n",
    "        self.jsonData = None\n",
    "\n",
    "\n",
    "        ## get kategorie \n",
    "        self.kategorie = self.url.replace('https://www.nius.de/','').split('/')[0]\n",
    "\n",
    "        ## create folder and files\n",
    "        if os.path.isdir(self.folderPath) == False:\n",
    "            os.mkdir(self.folderPath)\n",
    "        \n",
    "        if os.path.exists('error.log') == False:\n",
    "            with open('error.log',\"w\")as file:\n",
    "                file.write('')\n",
    "\n",
    "\n",
    "\n",
    "    def get_article_json(self):\n",
    "        \n",
    "        apiPathDict = {'news':'article','politik':'article','kommentar':'article','gesellschaft':'article','wirtschaft':'article',\n",
    "                       'clips':'clips','episodes':'episodes','ausland':'article','energie':'article','analyse':'article','berlin':'article',\n",
    "                       'corona':'article','gastbeitrag':'article','gesellschaft':'article','interview':'article','kriminalitaet':'article','leserbrief':'article',\n",
    "                       'medien':'article','nachrichten':'article','nius-live':'article','nius-originals':'article','show':'article','sport':'article',\n",
    "                       'statistik':'article','unterhaltung':'article','verbrechen':'article','wirtschaft':'article',}\n",
    "\n",
    "\n",
    "        if self.jsonData != None:\n",
    "            return self.jsonData\n",
    "        articleId = self.url.split('/')[-1]\n",
    "        jsonUrl = \"https://api.nius.de/\"+apiPathDict[self.kategorie]+\"/\"+articleId\n",
    "        try:\n",
    "            anfrage = requests.get(jsonUrl)\n",
    "        except:\n",
    "            with open('error.log','a') as file:\n",
    "                file.write(f'es gab ein fehler bei: {self.url},{datetime.now()}\\n')\n",
    "                return\n",
    "        if anfrage.status_code == 200:\n",
    "            self.jsonData = anfrage.json()\n",
    "        else:\n",
    "            print(f'no json found, statuscode: {anfrage.status_code}')\n",
    "        return(self.jsonData)\n",
    "    \n",
    "    def save_json(self, overWrite = False):\n",
    "        jsonData = self.get_article_json()\n",
    "        if jsonData == None:\n",
    "            return\n",
    "        if overWrite == False:\n",
    "            if os.path.exists(self.folderPath+jsonData['id']+'.json') == True:\n",
    "                print(f\"datei {self.folderPath+jsonData['id']+'.json'} existiert bereits\")\n",
    "                return\n",
    "        with open(self.folderPath+jsonData['id']+'.json','w') as file:\n",
    "            file.write(json.dumps(jsonData))\n",
    "        print(f\"saved {jsonData['id']}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## findet alle artikel aus der sidemap und speichert sie in der Datei\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "\n",
    "# URL der Sitemap\n",
    "sitemap_url = 'https://www.nius.de/sitemap.xml'\n",
    "\n",
    "# Sitemap herunterladen\n",
    "response = requests.get(sitemap_url)\n",
    "response.raise_for_status()  # Überprüft, ob der Request erfolgreich war\n",
    "\n",
    "# Sitemap parsen\n",
    "sitemap = ET.fromstring(response.content)\n",
    "\n",
    "# Alle <loc>-Tags extrahieren\n",
    "loc_tags = sitemap.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}loc')\n",
    "loc_urls = [loc.text for loc in loc_tags]\n",
    "\n",
    "csv_filename = 'sitemap_urls_29_11_24.csv'\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for url in loc_urls:\n",
    "        csvwriter.writerow([url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get articel links throug api \n",
    "count = 0\n",
    "api = f'https://api.nius.de/articles?text=&skip0=&take=10&videoOnly=false'\n",
    "text = requests.get(api).json()\n",
    "\n",
    "while len(text['results']) != 0:\n",
    "    api = f'https://api.nius.de/articles?text=&skip{count*2000}=&take=50000&videoOnly=false'\n",
    "    text = requests.get(api).json()\n",
    "    count +=1\n",
    "    file_path = f'articel_link_throug_api/{count}.json'\n",
    "    with open(file_path,'w') as file:\n",
    "        file.write(json.dumps(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract links out of api requests\n",
    "folder = 'articel_link_throug_api/'\n",
    "all_links = []\n",
    "for file in os.listdir(folder):\n",
    "    with open(folder+file,'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for i in data['results']:\n",
    "        link = f'https://www.nius.de{(i['articleRef'])}'\n",
    "        all_links.append(link)\n",
    "csv_filename = 'sitemap_urls_29_11_24.csv'\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for url in all_links:\n",
    "        csvwriter.writerow([url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(all_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rennt über die liste der urls aus der sitemap datei und speichert alle neuen\n",
    "listOfUrls = []\n",
    "with open ('sitemap_urls_29_11_24.csv','r') as file:\n",
    "    csvFile = csv.reader(file)\n",
    "    _ = [listOfUrls.append(i[0]) for i in csvFile]\n",
    "\n",
    "df = pd.read_csv('craweld_urls.csv', index_col='id')\n",
    "counter = 0\n",
    "\n",
    "for url in listOfUrls:\n",
    "    if df.url.str.contains(url,regex=False).any() == False:\n",
    "            counter += 1\n",
    "            seite(url).save_json()\n",
    "            df.loc[len(df.index)] = [url,datetime.now()]\n",
    "            if counter%50 == 0:\n",
    "                df.to_csv('craweld_urls.csv')\n",
    "            time.sleep(1)\n",
    "df.to_csv('craweld_urls.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
