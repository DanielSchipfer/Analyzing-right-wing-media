{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(info):\n",
    "    empty = [[],{},'',()]\n",
    "    if info == None or info == False:\n",
    "        return 'NaN'\n",
    "    if info in empty:\n",
    "        return  'NaN'\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'nius data/'\n",
    "allFiles = os.listdir(path)\n",
    "df = pd.DataFrame(columns=['id', 'title', 'content', 'tags','autor','kategorie','date'])\n",
    "\n",
    "\n",
    "for singleFile in allFiles:\n",
    "    oneFile = os.path.join(path,singleFile)\n",
    "    with open(oneFile,\"r\") as file:\n",
    "        data = json.load(file)\n",
    "        autor = np.NaN\n",
    "        if data['authors'] != []:\n",
    "            autor = data['authors'][0]['name']\n",
    "        kategoie = np.NaN\n",
    "        if data['categories'] != []:\n",
    "            kategoie = data['categories'][0]\n",
    "\n",
    "        date = data['meta']['publishedAt'].replace(data['meta']['publishedAt'].split(':')[-1],'')\n",
    "        df.loc[len(df)] =[file.name,data['data']['title'],data['data']['content'],data['data']['tags'],autor,kategoie,pd.to_datetime(date, format=\"%Y-%m-%dT%H:%M:\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all data for articles tabel\n",
    "article_df = pd.DataFrame(columns=['id','title','content','slug','pulic_author'])\n",
    "path = 'data/'\n",
    "allFiles = os.listdir(path)\n",
    "na = np.NaN\n",
    "\n",
    "for singleFile in allFiles:\n",
    "    oneFile = os.path.join(path,singleFile)\n",
    "    with open(oneFile,\"r\") as file:\n",
    "        data = json.load(file)\n",
    "        id = na\n",
    "        if data['id'] != '':\n",
    "            id = data['id']\n",
    "        title = na\n",
    "        if data['data']['title'] != '':\n",
    "            title = data['data']['title']\n",
    "        content = na\n",
    "        if data['data']['content'] != []:\n",
    "            content = data['data']['content']\n",
    "        slug = na\n",
    "        if data['articleRef'] != '':\n",
    "            slug = data['articleRef']\n",
    "        public_author = na\n",
    "        if data['author'] != '':\n",
    "            public_author = data['author']\n",
    "        article_df.loc[len(article_df)] =[id,title,content,slug,public_author]\n",
    "\n",
    "        #date = data['meta']['publishedAt'].replace(data['meta']['publishedAt'].split(':')[-1],'')\n",
    "        #df.loc[len(df)] =[file.name,data['data']['title'],data['data']['content'],data['data']['tags'],autor,kategoie,pd.to_datetime(date, format=\"%Y-%m-%dT%H:%M:\")]\n",
    "article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all data for articles tabel\n",
    "article_df = pd.DataFrame(columns=['id','title','content','slug'])\n",
    "path = 'nius data/'\n",
    "allFiles = os.listdir(path)\n",
    "na = np.NaN\n",
    "\n",
    "for singleFile in allFiles:\n",
    "    oneFile = os.path.join(path,singleFile)\n",
    "    with open(oneFile,\"r\") as file:\n",
    "        data = json.load(file)\n",
    "        id = na\n",
    "        if data['id'] != '':\n",
    "            id = data['id']\n",
    "        title = na\n",
    "        if data['data']['title'] != '':\n",
    "            title = data['data']['title']\n",
    "        content = na\n",
    "        if data['data']['content'] != []:\n",
    "            content = data['data']['content']\n",
    "        slug = na\n",
    "        if data['articleRef'] != '':\n",
    "            slug = data['articleRef']\n",
    "\n",
    "        article_df.loc[len(article_df)] =[id,title,content,slug]\n",
    "\n",
    "        #date = data['meta']['publishedAt'].replace(data['meta']['publishedAt'].split(':')[-1],'')\n",
    "        #df.loc[len(df)] =[file.name,data['data']['title'],data['data']['content'],data['data']['tags'],autor,kategoie,pd.to_datetime(date, format=\"%Y-%m-%dT%H:%M:\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all kategroies \n",
    "import numpy as np\n",
    "kategoie_df = pd.DataFrame(columns=['id','kategorie'])\n",
    "path = 'nius data/'\n",
    "allFiles = os.listdir(path)\n",
    "na = np.NaN\n",
    "\n",
    "for singleFile in allFiles:\n",
    "    oneFile = os.path.join(path,singleFile)\n",
    "    with open(oneFile,\"r\") as file:\n",
    "        data = json.load(file)\n",
    "        id = get_info(data['id'])\n",
    "        kategorie = get_info(data['meta']['category'])\n",
    "\n",
    "        kategoie_df.loc[len(kategoie_df)] =[id,kategorie]\n",
    "kategoie_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all autoren \n",
    "import numpy as np\n",
    "author_df = pd.DataFrame(columns=['autor_id','article_id','user_name','email','role'])\n",
    "path = 'nius data/'\n",
    "allFiles = os.listdir(path)\n",
    "na = np.NaN\n",
    "len_authors = []\n",
    "\n",
    "for singleFile in allFiles:\n",
    "    oneFile = os.path.join(path,singleFile)\n",
    "    with open(oneFile,\"r\") as file:\n",
    "        data = json.load(file)\n",
    "        article_id = get_info(data['id'])\n",
    "        if data['authors'] != []:\n",
    "            len_authors.append(len(data['authors']))\n",
    "            for author in data['authors']:\n",
    "                author_id = get_info(author['id'])\n",
    "                user_name = get_info(author['name'])\n",
    "                email = get_info(author['email'])\n",
    "                role = get_info(author['role'])\n",
    "                author_df.loc[len(author_df)] =[author_id, article_id, user_name, email, role]\n",
    "author_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get meta_data\n",
    "\n",
    "meta_data_df = pd.DataFrame(columns=['article_id','meta_title','description','image','locale','date_published','date_modified'])\n",
    "path = 'nius data/'\n",
    "allFiles = os.listdir(path)\n",
    "na = np.NaN\n",
    "\n",
    "for singleFile in allFiles:\n",
    "    oneFile = os.path.join(path,singleFile)\n",
    "    with open(oneFile,\"r\") as file:\n",
    "        data = json.load(file)\n",
    "        id = get_info(data['id'])\n",
    "        if data['meta'] != {}:\n",
    "            meta = data['meta']\n",
    "            meta_title = get_info(meta['title'])\n",
    "            meta_description = get_info(meta['description'])\n",
    "            image = get_info(meta['image'])\n",
    "            locale = get_info(meta['locale'])\n",
    "            date_published = get_info(meta[\"publishedAt\"])\n",
    "            date_modified = get_info(meta['modifiedAt'])\n",
    "\n",
    "            meta_data_df.loc[len(meta_data_df)] =[id,meta_title,meta_description, image,locale, date_published, date_modified]\n",
    "meta_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get meta_data\n",
    "meta_image_df = pd.DataFrame(columns=['article_id','image'])\n",
    "path = 'nius data/'\n",
    "allFiles = os.listdir(path)\n",
    "na = np.NaN\n",
    "\n",
    "for singleFile in allFiles:\n",
    "    oneFile = os.path.join(path,singleFile)\n",
    "    with open(oneFile,\"r\") as file:\n",
    "        data = json.load(file)\n",
    "        id = get_info(data['id'])\n",
    "        if data['meta'] != {}:\n",
    "            image = get_info(meta['image'])\n",
    "\n",
    "\n",
    "            meta_image_df.loc[len(meta_image_df)] =[id,image,]\n",
    "meta_image_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Funktion zum Umgang mit leeren Werten\n",
    "def get_info(info):\n",
    "    empty = [[], {}, '', ()]\n",
    "    if info is None or info is False or info in empty:\n",
    "        return 'NaN'\n",
    "    return info\n",
    "\n",
    "# Datenpfad\n",
    "path = 'data/'\n",
    "allFiles = os.listdir(path)\n",
    "\n",
    "# Liste f√ºr kombinierten DataFrame\n",
    "combined_data = []\n",
    "\n",
    "# Daten aus JSON-Dateien extrahieren\n",
    "for singleFile in allFiles:\n",
    "    oneFile = os.path.join(path, singleFile)\n",
    "    with open(oneFile, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "        # Basis-Informationen\n",
    "        id = get_info(data.get('id'))\n",
    "        \n",
    "        # Meta-Daten\n",
    "        meta = data.get('meta', {})\n",
    "        meta_title = get_info(meta.get('title'))\n",
    "        meta_description = get_info(meta.get('description'))\n",
    "        image = get_info(meta.get('image'))\n",
    "        locale = get_info(meta.get('locale'))\n",
    "        date_published = get_info(meta.get('publishedAt'))\n",
    "        date_modified = get_info(meta.get('modifiedAt'))\n",
    "        kategorie = get_info(meta.get('category'))\n",
    "        \n",
    "        # Artikel-Daten\n",
    "        data_content = data.get('data', {})\n",
    "        title = get_info(data_content.get('title'))\n",
    "        content = get_info(data_content.get('content'))\n",
    "        slug = get_info(data.get('articleRef'))\n",
    "        public_author = get_info(data.get('author'))\n",
    "\n",
    "        ## Autor Daten\n",
    "        for author in data['authors']:\n",
    "            author_id = get_info(author.get('id'))\n",
    "            user_name = get_info(author.get('name'))\n",
    "            email = get_info(author.get('email'))\n",
    "            role = get_info(author.get('role'))\n",
    "            # Daten kombinieren\n",
    "            combined_data.append({\n",
    "                'id': id,\n",
    "                'meta_title': meta_title,\n",
    "                'description': meta_description,\n",
    "                'image': image,\n",
    "                'locale': locale,\n",
    "                'date_published': date_published,\n",
    "                'date_modified': date_modified,\n",
    "                'kategorie': kategorie,\n",
    "                'title': title,\n",
    "                'content': content,\n",
    "                'slug': slug,\n",
    "                'public_author': public_author,\n",
    "                'author_id': author_id,\n",
    "                'author_user_name':user_name,\n",
    "                'author_email': email,\n",
    "                'author_role': role,\n",
    "\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "        if data.get('authors')== []: \n",
    "            # Daten kombinieren\n",
    "            combined_data.append({\n",
    "                'id': id,\n",
    "                'meta_title': meta_title,\n",
    "                'description': meta_description,\n",
    "                'image': image,\n",
    "                'locale': locale,\n",
    "                'date_published': date_published,\n",
    "                'date_modified': date_modified,\n",
    "                'kategorie': kategorie,\n",
    "                'title': title,\n",
    "                'content': content,\n",
    "                'slug': slug,\n",
    "                'public_author': public_author,\n",
    "                'author_id': np.NaN,\n",
    "                'author_user_name':np.NaN,\n",
    "                'author_email': np.NaN,\n",
    "                'author_role': np.NaN,\n",
    "\n",
    "            })\n",
    "            \n",
    "\n",
    "# Kombinierter DataFrame erstellen\n",
    "combined_df = pd.DataFrame(combined_data)\n",
    "combined_df['content'] = [f\"'{i}'\" for i in combined_df['content']]\n",
    "\n",
    "# Spezifische DataFrames erstellen\n",
    "meta_image_df = combined_df[['id', 'image']].dropna()\n",
    "meta_data_df = combined_df[['id', 'meta_title', 'description', 'image', 'locale', 'date_published', 'date_modified']].dropna()\n",
    "kategoie_df = combined_df[['id', 'kategorie']].dropna()\n",
    "article_df = combined_df[['id', 'title', 'content', 'slug', 'public_author']].dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "####### Category table ####\n",
    "###########################\n",
    "import mysql.connector\n",
    "\n",
    "# Tabellenname\n",
    "table_name = 'category'\n",
    "\n",
    "# Daten vorbereiten\n",
    "data = combined_df[['kategorie']].drop_duplicates()\n",
    "data_to_insert = [tuple(row) for row in data.to_numpy(na_value='NaN')]\n",
    "\n",
    "# Verbindung zur Datenbank herstellen\n",
    "mydb = mysql.connector.connect(\n",
    "    ##############\n",
    ")\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tabelle l√∂schen, falls sie existiert\n",
    "mycursor.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "# Tabelle erstellen\n",
    "create_table = f\"\"\"\n",
    "CREATE TABLE {table_name} (\n",
    "    `id` INT NOT NULL AUTO_INCREMENT,\n",
    "    `name` VARCHAR(100) NOT NULL,\n",
    "    PRIMARY KEY (`id`)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n",
    "\"\"\"\n",
    "mycursor.execute(create_table)\n",
    "\n",
    "# Daten einf√ºgen\n",
    "insert_query = f\"INSERT INTO {table_name} (name) VALUES (%s)\"\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(0, len(data_to_insert), batch_size):\n",
    "    mycursor.executemany(insert_query, data_to_insert[i:i + batch_size])\n",
    "    print(f\"Inserted rows {i + 1} to {min(i + batch_size, len(data_to_insert))}\")\n",
    "#\n",
    "# √Ñnderungen speichern\n",
    "mydb.commit()\n",
    "print(f\"Inserted {len(data_to_insert)} rows into {table_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "####### content table ####\n",
    "###########################\n",
    "import mysql.connector\n",
    "\n",
    "tabel_name = 'article_content'\n",
    "#data = combined_df[['title','content','public_author','kategorie']].drop_duplicates()\n",
    "#data_to_insert = [tuple(row) for row in data.to_numpy(na_value='NaN')]\n",
    "\n",
    "mydb = mysql.connector.connect(############)\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "\n",
    "## get categroy id \n",
    "mycursor.execute(\"SELECT id,name FROM category\")\n",
    "category_id_lookup = {name:id for id , name in mycursor.fetchall()}\n",
    "data = combined_df[['title','content','public_author','kategorie']].drop_duplicates()\n",
    "data['kategorie'] = [category_id_lookup.get(name) for name in data['kategorie']]\n",
    "data_to_insert = [tuple(row) for row in data.to_numpy(na_value='NaN')]\n",
    "\n",
    "\n",
    "## drpo table if exist\n",
    "mycursor.execute(f\"DROP TABLE IF EXISTS {tabel_name}\")\n",
    "## creat table\n",
    "create_table = f\"\"\" CREATE TABLE {tabel_name} (\n",
    "  `id` INT NOT NULL AUTO_INCREMENT,\n",
    "  `title` VARCHAR(250) NOT NULL,\n",
    "  `content` LONGTEXT NOT NULL,\n",
    "  `public_author` VARCHAR(100) NOT NULL,\n",
    "  `kategorie_id` INT NOT NULL,\n",
    "  PRIMARY KEY (`id`),\n",
    "  KEY `kategorie_id` (`kategorie_id`),\n",
    "  CONSTRAINT `fk_kategorie_id` FOREIGN KEY (`kategorie_id`) \n",
    "    REFERENCES `category` (`id`) \n",
    "    ON DELETE RESTRICT ON UPDATE RESTRICT\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n",
    "\"\"\"\n",
    "mycursor.execute(create_table)\n",
    "\n",
    "# Daten einf√ºgen\n",
    "insert_query = f\"\"\"INSERT INTO {tabel_name} (title,content, public_author,kategorie_id) \n",
    "                VALUES (%s,%s,%s,%s)\"\"\"\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(0, len(data_to_insert), batch_size):\n",
    "    mycursor.executemany(insert_query, data_to_insert[i:i + batch_size])\n",
    "    print(f\"Inserted rows {i + 1} to {min(i + batch_size, len(data_to_insert))}\")\n",
    "#\n",
    "# √Ñnderungen speichern\n",
    "mydb.commit()\n",
    "print(f\"Inserted {len(data_to_insert)} rows into {tabel_name}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "####### adding column ####\n",
    "###########################\n",
    "import mysql.connector\n",
    "\n",
    "# Verbindung zur Datenbank herstellen\n",
    "tabel_name = 'article_content'\n",
    "mydb = mysql.connector.connect(\n",
    "    ##################\n",
    ")\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "# Neue Spalte hinzuf√ºgen\n",
    "alter_table_query = f\"\"\"\n",
    "    ALTER TABLE {tabel_name}\n",
    "    ADD COLUMN cleaned_content LONGTEXT;\n",
    "\"\"\"\n",
    "#mycursor.execute(alter_table_query)\n",
    "#mydb.commit()\n",
    "print(f\"Spalte 'cleaned_content' wurde zur Tabelle '{tabel_name}' hinzugef√ºgt.\")\n",
    "import pandas as pd\n",
    "df = pd.read_csv('cleaned_data_article_content.csv',index_col='id')\n",
    "\n",
    "batch_size = 20\n",
    "ids = df.index.tolist()\n",
    "\n",
    "for start in range(0, len(ids), batch_size):\n",
    "    end = start + batch_size\n",
    "    batch_ids = ids[start:end]\n",
    "    print(f\"first: {start}\",end='\\r')\n",
    "    for i in batch_ids:\n",
    "        row = df.loc[i]\n",
    "        update_query = f\"\"\"\n",
    "            UPDATE {tabel_name}\n",
    "            SET cleaned_content = %s\n",
    "            WHERE id = %s;\n",
    "        \"\"\"\n",
    "        mycursor.execute(update_query, (row[\"content\"], i))\n",
    "    mydb.commit()\n",
    "print(\"Die Spalte 'cleaned_content' wurde mit den neuen Daten gef√ºllt.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "####### autor ############\n",
    "###########################\n",
    "import mysql.connector\n",
    "\n",
    "tabel_name = 'author'\n",
    "#data = combined_df[['title','content','public_author','kategorie']].drop_duplicates()\n",
    "#data_to_insert = [tuple(row) for row in data.to_numpy(na_value='NaN')]\n",
    "\n",
    "mydb = mysql.connector.connect(############)\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "\n",
    "data = combined_df[['author_user_name','author_email','author_role']].drop_duplicates()\n",
    "data_to_insert = [tuple(row) for row in data.to_numpy(na_value='NaN')]\n",
    "\n",
    "\n",
    "## drpo table if exist\n",
    "mycursor.execute(f\"DROP TABLE IF EXISTS {tabel_name}\")\n",
    "## creat table\n",
    "create_table = f\"\"\" CREATE TABLE {tabel_name} (\n",
    "  `id` INT NOT NULL AUTO_INCREMENT,\n",
    "  `user_name` VARCHAR(250) NOT NULL,\n",
    "  `email` VARCHAR(250) NOT NULL,\n",
    "  `role` VARCHAR(100) NOT NULL,\n",
    "  PRIMARY KEY (`id`)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n",
    "\"\"\"\n",
    "mycursor.execute(create_table)\n",
    "\n",
    "# Daten einf√ºgen\n",
    "insert_query = f\"\"\"INSERT INTO {tabel_name} (user_name,email, role) \n",
    "                VALUES (%s,%s,%s)\"\"\"\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(0, len(data_to_insert), batch_size):\n",
    "    mycursor.executemany(insert_query, data_to_insert[i:i + batch_size])\n",
    "    print(f\"Inserted rows {i + 1} to {min(i + batch_size, len(data_to_insert))}\")\n",
    "#\n",
    "# √Ñnderungen speichern\n",
    "mydb.commit()\n",
    "print(f\"Inserted {len(data_to_insert)} rows into {tabel_name}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "####### image table ####\n",
    "###########################\n",
    "import mysql.connector\n",
    "\n",
    "# Tabellenname\n",
    "table_name = 'image'\n",
    "\n",
    "# Daten vorbereiten\n",
    "data = combined_df[['image']].drop_duplicates()\n",
    "data_to_insert = [tuple(row) for row in data.to_numpy(na_value='NaN')]\n",
    "\n",
    "# Verbindung zur Datenbank herstellen\n",
    "mydb = mysql.connector.connect(\n",
    "    #########\n",
    ")\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tabelle l√∂schen, falls sie existiert\n",
    "mycursor.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "# Tabelle erstellen\n",
    "create_table = f\"\"\"\n",
    "CREATE TABLE {table_name} (\n",
    "    `id` INT NOT NULL AUTO_INCREMENT,\n",
    "    `link` VARCHAR(500) NOT NULL,\n",
    "    PRIMARY KEY (`id`)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n",
    "\"\"\"\n",
    "mycursor.execute(create_table)\n",
    "\n",
    "# Daten einf√ºgen\n",
    "insert_query = f\"INSERT INTO {table_name} (link) VALUES (%s)\"\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(0, len(data_to_insert), batch_size):\n",
    "    mycursor.executemany(insert_query, data_to_insert[i:i + batch_size])\n",
    "    print(f\"Inserted rows {i + 1} to {min(i + batch_size, len(data_to_insert))}\")\n",
    "#\n",
    "# √Ñnderungen speichern\n",
    "mydb.commit()\n",
    "print(f\"Inserted {len(data_to_insert)} rows into {table_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########################\n",
    "####### meta table #######\n",
    "###########################\n",
    "import mysql.connector\n",
    "\n",
    "tabel_name = 'meta_info'\n",
    "#data = combined_df[['title','content','public_author','kategorie']].drop_duplicates()\n",
    "#data_to_insert = [tuple(row) for row in data.to_numpy(na_value='NaN')]\n",
    "\n",
    "mydb = mysql.connector.connect(############)\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "\n",
    "## get categroy id \n",
    "print('prepare lookup table',end='\\r')\n",
    "mycursor.execute(\"SELECT id,content FROM article_content\")\n",
    "content_id_lookup = {name:id for id , name in mycursor.fetchall()}\n",
    "mycursor.execute(\"SELECT id,link FROM image\")\n",
    "image_id_lookup = {name:id for id , name in mycursor.fetchall()}\n",
    "data = combined_df[['slug','image','locale','date_published','date_modified','content']].drop_duplicates()\n",
    "data['content'] = [content_id_lookup.get(i)for i in data['content']]\n",
    "data['image'] =  [image_id_lookup.get(i) for i in data['image']]\n",
    "data['date_published'] = pd.to_datetime(data['date_published'], errors='coerce').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "data['date_modified'] = pd.to_datetime(data['date_modified'], errors='coerce').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "data_to_insert = [tuple(row) for row in data.to_numpy(na_value=None)]\n",
    "print('finished lookup table',end='\\r')\n",
    "\n",
    "\n",
    "## drpo table if exist\n",
    "mycursor.execute(f\"DROP TABLE IF EXISTS {tabel_name}\")\n",
    "## creat table\n",
    "create_table = f\"\"\" CREATE TABLE {tabel_name} (\n",
    "  `id` INT NOT NULL AUTO_INCREMENT,\n",
    "  `slug` TEXT NOT NULL,\n",
    "  `image_id` INT NOT NULL,\n",
    "  `locale` VARCHAR(250) NOT NULL,\n",
    "  `date_published` datetime ,\n",
    "  `date_modified` datetime ,\n",
    "  `content_id` int NOT NULL,\n",
    "  PRIMARY KEY (`id`),\n",
    "  KEY `content_id` (`content_id`),\n",
    "  KEY `image_id` (`image_id`),\n",
    "  CONSTRAINT `fk_content_id` FOREIGN KEY (`content_id`) \n",
    "    REFERENCES `article_content` (`id`) \n",
    "    ON DELETE RESTRICT ON UPDATE RESTRICT,\n",
    "  CONSTRAINT `fk_image_id` FOREIGN KEY (`image_id`) \n",
    "    REFERENCES `image` (`id`) \n",
    "    ON DELETE RESTRICT ON UPDATE RESTRICT\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n",
    "\"\"\"\n",
    "mycursor.execute(create_table)\n",
    "\n",
    "# Daten einf√ºgen\n",
    "insert_query = f\"\"\"INSERT INTO {tabel_name} (slug,image_id,locale, date_published,date_modified,content_id) \n",
    "                VALUES (%s,%s,%s,%s,%s,%s)\"\"\"\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(0, len(data_to_insert), batch_size):\n",
    "    mycursor.executemany(insert_query, data_to_insert[i:i + batch_size])\n",
    "    print(f\"Inserted rows {i + 1} to {min(i + batch_size, len(data_to_insert))}\")\n",
    "#\n",
    "# √Ñnderungen speichern\n",
    "mydb.commit()\n",
    "print(f\"Inserted {len(data_to_insert)} rows into {tabel_name}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########################\n",
    "####### author_mapping table #######\n",
    "###########################\n",
    "import mysql.connector\n",
    "\n",
    "tabel_name = 'author_mapping'\n",
    "#data = combined_df[['title','content','public_author','kategorie']].drop_duplicates()\n",
    "#data_to_insert = [tuple(row) for row in data.to_numpy(na_value='NaN')]\n",
    "\n",
    "mydb = mysql.connector.connect(############)\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "\n",
    "## get categroy id \n",
    "print('prepare lookup table',end='\\r')\n",
    "mycursor.execute(\"SELECT id,user_name FROM author\")\n",
    "author_id_lookup = {name:id for id , name in mycursor.fetchall()}\n",
    "mycursor.execute(\"SELECT id,slug  FROM meta_info\")\n",
    "meta_id_lookup = {name:id for id , name in mycursor.fetchall()}\n",
    "\n",
    "data = combined_df[['slug','author_user_name']].drop_duplicates()\n",
    "data['author_user_name'] = data['author_user_name'].replace(np.NaN,'NaN')\n",
    "data['author_user_name'] = [author_id_lookup.get(i)for i in data['author_user_name']]\n",
    "data['slug'] =  [meta_id_lookup.get(i) for i in data['slug']]\n",
    "data_to_insert = [tuple((int(row[0]),int(row[1]))) for row in data.to_numpy()]\n",
    "print('finished lookup table',end='\\r')\n",
    "\n",
    "\n",
    "## drpo table if exist\n",
    "mycursor.execute(f\"DROP TABLE IF EXISTS {tabel_name}\")\n",
    "## creat table\n",
    "create_table = f\"\"\" CREATE TABLE {tabel_name} (\n",
    "  `id` INT NOT NULL AUTO_INCREMENT,\n",
    "  `meta_id` INT NOT NULL,\n",
    "  `author_id` INT NOT NULL,\n",
    "  PRIMARY KEY (`id`),\n",
    "  KEY `meta_id` (`meta_id`),\n",
    "  KEY `author_id` (`author_id`),\n",
    "  CONSTRAINT `fk_meta_id` FOREIGN KEY (`meta_id`) \n",
    "    REFERENCES `meta_info` (`id`) \n",
    "    ON DELETE RESTRICT ON UPDATE RESTRICT,\n",
    "  CONSTRAINT `fk_author_id` FOREIGN KEY (`author_id`) \n",
    "    REFERENCES `author` (`id`) \n",
    "    ON DELETE RESTRICT ON UPDATE RESTRICT\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n",
    "\"\"\"\n",
    "mycursor.execute(create_table)\n",
    "\n",
    "# Daten einf√ºgen\n",
    "insert_query = f\"\"\"INSERT INTO {tabel_name} (meta_id, author_id) \n",
    "                VALUES (%s,%s)\"\"\"\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(0, len(data_to_insert), batch_size):\n",
    "    mycursor.executemany(insert_query, data_to_insert[i:i + batch_size])\n",
    "    print(f\"Inserted rows {i + 1} to {min(i + batch_size, len(data_to_insert))}\")\n",
    "#\n",
    "# √Ñnderungen speichern\n",
    "mydb.commit()\n",
    "print(f\"Inserted {len(data_to_insert)} rows into {tabel_name}.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
